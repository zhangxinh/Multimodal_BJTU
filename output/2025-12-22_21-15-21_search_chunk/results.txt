command: python main.py search_chunk Transformer 的核心架构是什么？
query: Transformer 的核心架构是什么？

[0.677] /volume/pt-train/users/wzhang/rstao/zxh/github/Multimodal/papers/NLP_Natural_Language_Processing/1910.10683v4.pdf#page4: on the “Transformer” architecture (Vaswani et al., 2017). The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 
[0.655] /volume/pt-train/users/wzhang/rstao/zxh/github/Multimodal/papers/NLP_Natural_Language_Processing/Transformer.pdf#page3: Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left an
[0.654] /volume/pt-train/users/wzhang/rstao/zxh/github/Multimodal/papers/NLP_Natural_Language_Processing/Transformer.pdf#page2: current attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the T
[0.635] /volume/pt-train/users/wzhang/rstao/zxh/github/Multimodal/papers/NLP_Natural_Language_Processing/Transformer.pdf#page2: es, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21]
[0.629] /volume/pt-train/users/wzhang/rstao/zxh/github/Multimodal/papers/NLP_Natural_Language_Processing/Transformer.pdf#page10: used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better resu